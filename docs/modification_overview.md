# 近期改动梳理

下表按模块汇总了近期为了适配高空拦截新场景而提交的主要代码改动，并说明调整动机与带来的效果。

## 场景与初始条件

| 改动位置 | 做了什么 | 这样修改的原因 | 修改后的作用 |
| --- | --- | --- | --- |
| `Environment/reset_env.py`<br>`Environment/init_env.py` | 将飞机初始高度限定在 8–12 km，并在半径约 20 km 的水平椭圆带内、±3 km 的垂直范围内随机生成三枚来袭导弹的初始点位，同时把导弹初速提高到 2–3 马赫；飞机速度保持 0.5–1.2 马赫区间。 | 新需求要求在高空巡航时拦截远距离来袭导弹，必须扩大空间尺度和速度分布才能与 0.01 s 的步长匹配。 | 训练与验证环境在重置时就处于新的几何尺度内，观测和奖励的数值范围随之增大但保持一致，使得模型看到的轨迹与真实场景一致。【F:Environment/reset_env.py†L7-L74】【F:Environment/init_env.py†L10-L83】 |
| `Environment/env.py` | 为状态向量增加 `position_scale=25000.0` 的归一化尺度，重置时按新的导弹/拦截弹数量重新构造观测缓存。 | 在空间尺度扩大后，直接使用未缩放的位置会造成输入尺度过大，影响网络稳定性。 | 所有状态在进入网络前都被统一按 25 km 尺度和 π 角度范围无量纲化，避免数值爆炸，同时方便还原真实坐标用于仿真判定。【F:Environment/env.py†L55-L194】【F:Environment/env.py†L816-L889】 |

## 导弹与拦截弹动力学

| 改动位置 | 做了什么 | 这样修改的原因 | 修改后的作用 |
| --- | --- | --- | --- |
| `flat_models/trajectory.py` 中 `Missiles` 类 | 新增两段运动：5 s 内从初始速度线性拉升至 1200 m/s，之后每秒对当前速度乘以 0.99，并保证不低于 200 m/s。 | 贴合任务说明的“加速段 + 纯 PN 制导 + 周期性速度衰减”设定，同时防止速度降至不合理数值。 | 来袭导弹的速度剖面符合最新战术假设，PN 导引保持既有角速度更新逻辑，但速度变化更加真实，改善了奖励曲线的物理含义。【F:flat_models/trajectory.py†L55-L195】 |
| `flat_models/trajectory.py` 中 `Interceptor` 类 | 拦截弹沿用同样的两段运动逻辑，发射时通过 `begin_pursuit` 重置加速段并允许以飞机速度或预设拦截速度为起始值。 | 拦截弹需要与来袭导弹匹配的动态约束，否则拦截距离与时间预测会偏差。 | 发射瞬间可重新注入初始速度并自动进入加速段，随后同样每秒 0.99 衰减，使拦截时间轴与奖励计算保持一致并避免过快耗尽速度。【F:flat_models/trajectory.py†L571-L714】 |
| `Environment/env.py` | 拦截弹在待命时调用 `sync_with_aircraft` 与飞机共速共位，发射后通过新的两段运动模型更新位置，并在 20 m 内判断击杀。 | 需要在“加速段”期间正确同步飞机速度，并确保拦截判据与新的动力学一致。 | 发射窗口、剩余弹数和拦截成功条件与新速度模型协同工作，训练时更易学习何时发射及保持追踪。【F:Environment/env.py†L163-L191】【F:Environment/env.py†L900-L920】 |

## 强化学习配置

| 改动位置 | 做了什么 | 这样修改的原因 | 修改后的作用 |
| --- | --- | --- | --- |
| `DDQN/DQNAgent.py` | 为智能体构造函数增加最小 ε、目标网络更新步数和梯度裁剪等可配置参数，并在计算 Double-DQN 目标时使用目标网络估计 Q 值。 | 新场景下奖励幅度更大、训练不稳定，需要更慢的目标网络同步和梯度裁剪来抑制震荡。 | 训练时可通过命令行调整探索边界与目标同步节奏，梯度裁剪防止更新爆炸，目标估计更稳定，从而提升收敛性。【F:DDQN/DQNAgent.py†L24-L168】 |
| `utils/train.py` | 为训练脚本新增学习率、γ、目标网络更新频率、ε 调度和梯度裁剪的命令行参数，并把这些配置写入验证流程。 | 便于在新的时间尺度下快速做超参数扫描，无需改代码即可尝试不同日程。 | 训练日志和保存的配置保持一致，验证脚本复现训练时的折扣和剪裁设定，使多次实验的结果可比且可复现。【F:utils/train.py†L153-L235】 |
| `utils/validate.py` | 评估时根据保存的配置构建智能体，强制使用同一折扣、学习率、目标更新频率和裁剪阈值，且加载检查点后立即对齐目标网络。 | 新的 ε/目标更新设定如果在验证时未复用，会导致表现偏差。 | 验证阶段动作选择保持确定性，统计成功率时与训练策略一致，减少因配置漂移导致的评估噪声。【F:utils/validate.py†L18-L120】 |

以上调整相互配合，使得环境动力学、观测尺度与训练调度共同匹配新的高空拦截场景，后续在此基础上即可继续进行奖励或策略的细化调优。
